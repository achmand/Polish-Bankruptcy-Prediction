\subsection{Decision Tree}\label{ssec:decisiontree}
Decision trees is a model which is a widely used technique in machine learning as it is quite simple but very powerful. This model can be used for both regression and classification problems, and there are different types of decision trees, some of which are; ID3, C4.5, CART, CHAID and MARS. In this section the CART algorithm will be discussed (as a classification problem) since this is the implementation chosen for the experiments, but the concept behind this implementation is very close to the other algorithms (the main difference in these implementation is the way the split is handled).

\noindent CART (Classification and Regression Tree) \cite{breiman1984classification} is a binary tree which is built by taking a set of instances (training set) and passing them to the root node, then a decision is made (if statement) based on the features, so to partition/split the instances into two subsets and passing them as an input to the child nodes. This procedure is then applied to each node until it reaches a subset with the purest (no mixed classes) distributions of the classes, this is also known as the leaf node.        
\noindent The most important aspect of this procedure is to know which decision should be taken at each node to decrease the uncertainty (minimizing mixing of classes) of the subset. Firstly, a metric to quantify the uncertainty/purity at each node is needed and in this implementation Gini Impurity \cite{gini1912variabilita} is used as shown in Equation \eqref{eq:gini}. The formula takes the node’s class of each instance in the subset, for that node \(t\), and computes the probability of obtaining two different outputs from all the possible classes \(k\). If the subset only contains instances with the same label, the subset is pure, meaning \(I(t) = 0\). There are other methods which can be used to quantify the impurity at each node such as Entropy, MSE and MAE, but it depends on the problem being tackled (classification or regression).
\begin{multicols}{2}
    \begin{equation} \label{eq:gini}
        I(t) = 1 - \sum_{j=1}^{k} p^2(j|t) 
    \end{equation} 
    \\
    \begin{equation} \label{eq:infogain}
        \begin{aligned}
           I_g(t) = I(t) - \frac{N_{tR}}{N_t} I(t_R) - \frac{N_{tL}}{N_t} I(t_L) 
        \end{aligned}
    \end{equation}
\end{multicols}
\noindent As described above the way a node’s uncertainty is measured is defined, but another metric is needed to quantify how much a certain decision reduces uncertainty, and for this Information Gain is used. Firstly, lets define what a decision is, a decision is simply a condition based on a specific feature and it usually is just a $\geq$ check when it's numeric or $==$ check when it’s categorical. The Equation \eqref{eq:infogain} shows how information gain is computed by taking the $I(t)$ of the current node and subtracting it by the total number of samples in the right node over the total number of samples in the parent node \(\frac{N_{tR}}{N_t}\) multiplied by the right node uncertainty $I(t_R)$ and subtracting again with weighted average of left node uncertainty \(\frac{N_{tL}}{N_t} I(t_L) \) based on a specific decision. 

\noindent So, at each node every feature of each instance is checked using the Information Gain to find the best split, and once this is found the node is split into two child nodes taking as input the subset which met the criteria and the subset which did not meet the criteria. This is done recursively until there is no more further splits and by then the leaf nodes are reached.  Once the tree is built/trained, one can predict the class of unforeseen instances by passing the test dataset to the tree and it will follow down the tree taking the left or right nodes based on the criteria of the decision. Once a leaf node is reached the tree will predict the probability of that instance belonging to a specific class (the highest probability is outputted if there are multiple labels in the leaf node). A maximum depth for the Tree can be set or otherwise the Tree will keep on finding the best splits until all leaves are pure (if possible). 