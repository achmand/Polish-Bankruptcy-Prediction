\section{Cross Validation}\label{sec:cross_val_sec}
Validation is a technique used to evaluate and validate a modelâ€™s performance. The main concept behind the validation process is to partition the dataset into a training set, which is used to train the model, and testing set, which is used to test the model. In validation the training set is only utilized once just to train the model. When this technique is used, one must specify the percentage of the split which is usually set between 60:40 ratio or 70:30 ratio. 

\noindent The problem with splitting the dataset into two partitions is that the model will only give as how well it performed based on the data that was used to train it. Now this could lead to overfitting or underfitting the dataset and may not give a good indicator of how it will perform in a more generalized way (unforeseen data outside these two partitions).  Validation also tends to induce some testing bias since we reserved a piece of the dataset just for testing.  

\noindent To tackle such problems and fully utilize our dataset, cross-validation can be used. There are various techniques which try to ensure low bias and low variance such as K-Fold Cross Validation, Stratified K-Fold Cross Validation and Leave-P-Out Cross Validation. In this section we will be discussing K-Fold cross validation \cite{mosteller1968data} since it was decided to use this method in our experiments, as it is one of the most commonly used method for cross validation.   

\noindent In this technique the dataset available is split into k partitions, and k-1 partitions is used as a training set and the remaining partition is used as a testing set once. Then this process is iterated for k number of times, so in the end of the iterations every partition will be used once as a testing set. At each iteration/fold some performance metrics are measured and by the end of the iterations these metrics are averaged out. This reduces both bias and variance as the original dataset is used for both training and testing sets, and by doing so the model is neither overfitted due to having 1 large training set, nor is it being underfitted due to having a larger test set than the traditional validation method. The dataset prior to splitting can remain either contiguous or randomized.  

\noindent Moreover, this technique can be applied to multiple models to help in the selection of the best performing model by using the same folds for each model at each iteration. This also enables the facility to tune hyperparameters by using this cross-validation technique with the same model but different hyperparameters and selecting the best at the end of the iteration (using some performance metrics). The parameter \(k\) is usually set to \(k = 5\) or \(k = 10\) \cite{kuhn2013applied} as there is a bias-variance trade-off when choosing \(k\) and these are known to produce a good balance between the two. 