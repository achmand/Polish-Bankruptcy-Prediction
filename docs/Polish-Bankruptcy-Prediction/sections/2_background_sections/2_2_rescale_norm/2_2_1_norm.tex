\subsection{Normalisation}\label{ssec:normalisation}

Normalisation is a pre-processing step needed for some of the algorithms in machine learning to work or improve performance. Some examples are KNN (features having the same magnitude for distance metrics), PCA for feature reduction (when using correlation matrix), Chi2 for feature selection (doesn’t work with negative values so normalisation can rescale to positive values) or when using gradient descent (makes minimizing the cost more efficient). Equation \eqref{eq:normrescale} is used to rescale values to a specific range. It describes how a value can be rescaled within a specific range $x_{min}$ to $x_{max}$ (commonly set to 0.0 to 1.0). When you apply this rescaling technique to all set of features (using the same range), normalisation is achieved, and all features are in the same range and as shown in Subsection \ref{sssec:featmagnitude} the features in our datasets contain different magnitudes and negative values.
\begin{equation} \label{eq:normrescale}
    x_{new} = \frac{x - x_{min}}{x_{max} - x_{min}}
\end{equation}
\noindent In this description of normalisation, it is described as having different types of values within the same range although some may use this definition to describe the process of roughly transforming a set of values to a normal distribution such a Box-Cox transformation \cite{box1964analysis} or log transformation. In the pre-processing steps, normalisation to transform to normal distribution won’t be used since the datasets is highly skewed and normal distribution won’t be achieved even if well-known techniques were applied. Furthermore, rescaling all features to have the same set of ranges (0 to 1) will be used to apply feature selection techniques which require positive values and for Logistic Regression (for gradient descent as described above).                

