\section{Machine Learning Models}\label{ssec:proposedmodels}
For this problem three supervised machine learning algorithms will be used for classification, which are as follows; Decision Tree, Random Forest and Logistic Regression.

\noindent The Decision Tree model was firstly chosen because it supports classification. Such model supports both discrete and continuous data as features, and since the dataset contains continuous data only the Decision Tree model will work well. It supports multi class classification but for this problem only binary classification is needed. Another reason why the Decision Tree model was chosen for this project is that it is not affected by the magnitude/ranges of the features as shown is Subsection \ref{sssec:featmagnitude}.  It also does not assume that the features are normally distributed, since our dataset as shown in Subsection \ref{sssec:featdist} does not follow a normal distribution. 

\noindent Random Forest was chosen to be implemented and compared, since the Decision Tree model is susceptible to overfitting. This model will help with low bias and high variance found in Decision Trees when applied to a large dataset, with the use of ensemble techniques such as random sub space method and dataset bagging. Again, since Random Forest use an ensemble of Decision Trees (weak learners), it may help with the performance and again such model is not affected by the magnitude/range or assumes a normal distribution of the features. 

\noindent Lastly the Logistic Regression model will be used in the experiments since it is a binary classification model and due to dichotomous nature of the labels found in the dataset. Similar to the decision tree, this model does not assume a normal distribution in the features. Unlike the Decision Tree model, the Logistic Regression can be affected by the magnitude of the features when training the model (adjusting weights/gradient descent). The magnitude of the features can be normalized by mapping them to a specific range, which is usually set between 0.0 to 1.0. All the models described above assume that the dataset has balanced outcomes, since it may be subjected to outcome bias and will not perform well in unforeseen instances (minority class). SMOTE technique described in Subsection \ref{sssec:smote} can used to oversample the minority class. 
