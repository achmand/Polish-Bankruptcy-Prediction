\subsection{Decision Tree}\label{ssec:impldt}
For the implementation of Decision Tree two python scripts were created \textbf{decision\_tree\_criterion.pyx} and \textbf{decision\_tree.py}. The \textbf{decision\_tree\_criterion.pyx} have the following functions which were described in Subsection \ref{ssec:decisiontree}; \textit{binary\_gini\_impurity}, \textit{binary\_information\_gain} and \textit{decision\_split} which were later utilized in the \textbf{decision\_tree.py} script. In the \textbf{decision\_tree\_criterion.pyx} script, \textbf{cython} \cite{behnel2010cython} (python to c compiler) was utilized to achieve better performance hence the \textit{'.pyx'} format.  

\noindent Three classes can be found in the \textbf{decision\_tree.py} script which are as follows; \textit{LeafNode}, \textit{InternalNode} and \textit{DecisionTree}. Once a Decision Tree is initialized the \textit{fit} function is called to start the training phase. This function takes two parameters, the instances $X$ and their labels $y$. In turn, this function calls the \textit{construct\_tree} function to construct the tree using recursion. On the first iteration the \textit{get\_split} function is called which returns the best gain and decision. The returned best gain is checked and if the gain is equal to 0 a \textit{LeafNode} is returned. If not, the instances are split based on the best decision returned and the \textit{contruct\_tree} function is called on the split nodes (hence recursion). At the end of this recursive procedure an \textit{InternalNode} (root node) is returned with a reference to the first two nodes at the beginning of the recursion. Moreover, a heuristic was added in the \textit{get\_split} function so not every value of each feature is checked. This was done by sorting the values of the feature being checked and only take values which have different labels in the sequence of sorted values. Then the average of every two values in the sequence is used to test the split. Once the tree is trained the \textit{predict} function can be called to output an array of the predicted values based on the instances passed $x$.

\noindent The end implementation for Decision Tree used Gini Impurity to calculate uncertainty, Gini information Gain to quantify the best split using a particular feature, recursion to construct the tree and finding the best split at each node, and it keeps on adding nodes until pure leaf nodes (if possible) are found.