\subsection{Logistic Regression}\label{ssec:impllr}
The Logistic Regression implementation can be found in the script \textbf{logistic\_regression.py}. It inherits two \textbf{sklearn} \cite{python:sklearn_api} base classes named \textit{BaseEstimator} and \textit{ClassifierMixin}. This was done to integrate the Logstic Regression model with the \textbf{sklearn} RFE function as described in Section \ref{sec:processdataset} (for Feature Selection). All the logic for the training and predictions were implemented using first principles and these inherited classes were only used so that this model can be used as an estimator in some \textbf{sklearn} functions such as RFE. The class can be initialized with the following parameters; \textit{alpha} (the learning rate), \textit{max\_epochs} (the number of iterations), \textit{penalty} (L1 or L2 regularization), \textit{lambda\_t} (regularization term) and \textit{verbose} (to display stats in each iteration in Gradient Descent).

\noindent The \textit{fit} function uses Gradient Descent to minimize J(θ), where feature coefficients vector θ values are initialized to 0, and a bias vector initialized to 1 is added to each example’s coefficients. It also has the option to either use L1 or L2 regularization. To apply Gradient Descent the \textit{fit} function loops for a specific number of epoch as passed in \textit{max\_epochs} and adjust the weights/thetas using the method described in Subsection \ref{ssec:logreg}, with the specified \textit{alpha} as the learning rate for the Gradient Descent. The \textit{predict} function predicts the outcome of a particular company’s set of data, to determine if it will go bankrupt or not, by passing the features through the already trained logistic model and returning an array with the predicted classification.

\noindent In the Experiments the Logistic Regression model was initialized using an alpha of 0.01, max epochs of 1000, using Ridge Regression (L2) with regularization term lambda of 0.1.